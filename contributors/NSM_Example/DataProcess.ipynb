{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import time\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import tables\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import pickle \n",
    "from pickle import dump\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# Create .py function to process data to train model. \n",
    "\n",
    "def DataProcess(datapath, cwd):\n",
    "    #regions are N_Sierra, S_Sierra_Low, S_Sierra_High\n",
    "\n",
    "    Region_list = ['N_Sierras', 'S_Sierras_Low', 'S_Sierras_High']\n",
    "\n",
    "    #set data path\n",
    "    training_path = f\"{datapath}/data\"\n",
    "\n",
    "    #get region ids\n",
    "    #Regions = pd.read_csv(f\"{training_path}/Regions.csv\")\n",
    "    #Region_list =list(Regions['Regions'].values)\n",
    "\n",
    "    RegionTrain = {}\n",
    "    print('Processing training data regions into one dataframe')\n",
    "    for region in tqdm(Region_list):   \n",
    "        #load the RegionTrain DF, select the key grid cell colums, and add to training DF.\n",
    "        RegionTrain[region] = pd.read_hdf(f\"{training_path}/RegionTrain_SCA.h5\", region)\n",
    "\n",
    "    #load RFE optimized features\n",
    "    Region_optfeatures= pickle.load(open(f\"{training_path}/Optimal_Features.pkl\", \"rb\"))\n",
    "\n",
    "    #Split the data the same as original train/test split to make same figures/analysis\n",
    "    VIIRS_cols = ['Date', 'VIIRS_SCA', 'hasSnow']\n",
    "    RegionTrain_notScaled = {}\n",
    "    RegionObs = {}\n",
    "    RegionTest = {}\n",
    "    TestingYR = 2019\n",
    "\n",
    "    #correct testing year for WY\n",
    "    TestingYR = str(TestingYR-1)\n",
    "\n",
    "    for Region in Region_list:\n",
    "        #print(Region)\n",
    "\n",
    "        #Pull a specific Water Year out of the Training Data\n",
    "        dfTest = RegionTrain[Region][RegionTrain[Region]['Date'] >=f\"10-01-{TestingYR}\"].copy()\n",
    "        RegionTrain[Region] = RegionTrain[Region][RegionTrain[Region]['Date'] < f\"10-01-{TestingYR}\"]\n",
    "\n",
    "\n",
    "        #get y data\n",
    "        y = RegionTrain[Region]['SWE']\n",
    "\n",
    "        #get max SWE for normalization and prediction\n",
    "        SWEmax = max(RegionTrain[Region]['SWE'])\n",
    "        y = y/SWEmax\n",
    "\n",
    "        #get optimal features for each regions (from LGBM RFE), first pop off fSCA\n",
    "        optfeatures = list(Region_optfeatures[Region])\n",
    "\n",
    "        #make a df copy of specific region\n",
    "        df = RegionTrain.get(Region).copy()\n",
    "        dfVIIRS = df[VIIRS_cols].copy()\n",
    "        dfVIIRS.reset_index(inplace = True)\n",
    "        df = df[optfeatures]\n",
    "\n",
    "        ### replace special character ':' with '__' \n",
    "        df = df.rename(columns = lambda x:re.sub(':', '__', x))\n",
    "\n",
    "        #change all na values to prevent scaling issues\n",
    "        df[df< -9000]= -10    \n",
    "        df_notscaled = df.copy()\n",
    "        #normalize training data    \n",
    "        # normalize features\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        #save scaler data here too\n",
    "        dump(scaler, open(f\"{cwd}/Model/{Region}/{Region}_scaler.pkl', 'wb'))\n",
    "        scaled = scaler.fit_transform(df)\n",
    "        df = pd.DataFrame(scaled, columns = df.columns)\n",
    "\n",
    "        #Add Viirs colums\n",
    "        df = pd.concat([df, dfVIIRS], axis = 1)\n",
    "        df.set_index('index', inplace = True, drop = True)\n",
    "\n",
    "        #Set the 75/25% train/test split, set a random state to get train/test for future analysis\n",
    "        X = df\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1234)\n",
    "\n",
    "        #get non transformed data to support analysis\n",
    "        X_notscaled = df_notscaled\n",
    "        X_train_notscaled, X_test_notscaled, y_train_notscaled, y_test_notscaled = train_test_split(X_notscaled, y, test_size=0.25, random_state=1234)\n",
    "\n",
    "        #Set the RegionTrain and RegionOBS dicts\n",
    "        RegionTrain[Region] = X_test\n",
    "        RegionTrain_notScaled[Region] = X_test_notscaled\n",
    "        RegionObs[Region] = pd.DataFrame(y_test, columns = ['SWE'])\n",
    "        RegionTest[Region] = dfTest\n",
    "        \n",
    "        return RegionTrain, RegionTrain_notScaled, RegionObs, RegionTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eca94c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NSM_env",
   "language": "python",
   "name": "nsm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
